{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wgjt3CFgLucE",
        "outputId": "3e76be89-1db1-464d-fa76-4e9398f8b398"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Credit Card Fraud Detection - Preprocessing ===\n",
            "Libraries imported successfully!\n",
            "EDA Insights loaded:\n",
            "• fraud_rate: 0.26\n",
            "• imbalance_ratio: 384\n",
            "• top_features: ['V17', 'V14', 'V3', 'V10', 'V12', 'V16', 'V7', 'V11', 'V4', 'V18']\n",
            "• peak_fraud_hour: 2.09\n",
            "• total_transactions: 63472\n",
            "• duplicates_count: 260\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 1. SETUP AND LOAD EDA INSIGHTS\n",
        "# ============================================================================\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.combine import SMOTEENN\n",
        "from collections import Counter\n",
        "import joblib\n",
        "\n",
        "# Configuration\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"=== Credit Card Fraud Detection - Preprocessing ===\")\n",
        "print(\"Libraries imported successfully!\")\n",
        "\n",
        "# Load insights from EDA\n",
        "EDA_INSIGHTS = {\n",
        "    'fraud_rate': 0.26,\n",
        "    'imbalance_ratio': 384,\n",
        "    'top_features': ['V17', 'V14', 'V3', 'V10', 'V12', 'V16', 'V7', 'V11', 'V4', 'V18'],\n",
        "    'peak_fraud_hour': 2.09,\n",
        "    'total_transactions': 63472,\n",
        "    'duplicates_count': 260\n",
        "}\n",
        "\n",
        "print(\"EDA Insights loaded:\")\n",
        "for key, value in EDA_INSIGHTS.items():\n",
        "    print(f\"• {key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GdvE2K0M91G",
        "outputId": "3585d8d5-95cb-4a1a-e55a-bb10a0873a70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "DATA LOADING AND CLEANING\n",
            "==================================================\n",
            "Original dataset shape: (284807, 31)\n",
            "Duplicates found: 1081\n",
            "Dataset shape after removing duplicates: (283726, 31)\n",
            "Duplicates after cleaning: 0\n",
            "✓ Removed 1081 duplicate rows\n",
            "\n",
            "Class distribution after cleaning:\n",
            "• Normal (0): 283,253\n",
            "• Fraud (1): 473\n",
            "• Fraud rate: 0.1667%\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 2. DATA LOADING AND INITIAL CLEANING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)²\n",
        "print(\"DATA LOADING AND CLEANING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/creditcard.csv')\n",
        "print(f\"Original dataset shape: {df.shape}\")\n",
        "\n",
        "# Check for duplicates (we know there are 260 from EDA)\n",
        "duplicates_before = df.duplicated().sum()\n",
        "print(f\"Duplicates found: {duplicates_before}\")\n",
        "\n",
        "# Remove duplicates\n",
        "df_clean = df.drop_duplicates()\n",
        "duplicates_after = df_clean.duplicated().sum()\n",
        "print(f\"Dataset shape after removing duplicates: {df_clean.shape}\")\n",
        "print(f\"Duplicates after cleaning: {duplicates_after}\")\n",
        "print(f\"✓ Removed {duplicates_before} duplicate rows\")\n",
        "\n",
        "# Update our working dataframe\n",
        "df = df_clean.copy()\n",
        "\n",
        "# Verify class distribution after cleaning\n",
        "class_dist = df['Class'].value_counts()\n",
        "fraud_rate = df['Class'].mean() * 100\n",
        "print(f\"\\nClass distribution after cleaning:\")\n",
        "print(f\"• Normal (0): {class_dist[0]:,}\")\n",
        "print(f\"• Fraud (1): {class_dist[1]:,}\")\n",
        "print(f\"• Fraud rate: {fraud_rate:.4f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkq-8BWVNI9X",
        "outputId": "7ac7e0eb-a182-40c8-a0ba-b5fd2512e9e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "FEATURE ENGINEERING\n",
            "==================================================\n",
            "Creating time-based features...\n",
            "Creating amount-based features...\n",
            "Feature engineering completed!\n",
            "New dataset shape: (283726, 39)\n",
            "Added 8 new features\n",
            "New features: ['Hour', 'Day', 'Is_Night', 'Is_Peak_Fraud_Hour', 'Is_Weekend', 'Amount_Log', 'Is_Zero_Amount', 'Amount_Percentile']\n",
            "\n",
            "New features analysis:\n",
            "• Is_Night: Fraud rate when True: 0.2814%, when False: 0.1421%\n",
            "• Is_Peak_Fraud_Hour: Fraud rate when True: 0.6815%, when False: 0.1459%\n",
            "• Is_Zero_Amount: Fraud rate when True: 1.3827%, when False: 0.1589%\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 3. FEATURE ENGINEERING BASED ON EDA INSIGHTS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FEATURE ENGINEERING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Create a copy for feature engineering\n",
        "df_features = df.copy()\n",
        "\n",
        "# Time-based features (based on EDA insight: peak fraud at ~2 AM)\n",
        "print(\"Creating time-based features...\")\n",
        "df_features['Hour'] = (df_features['Time'] / 3600) % 24\n",
        "df_features['Day'] = (df_features['Time'] / (3600 * 24)).astype(int)\n",
        "\n",
        "# Binary features based on EDA insights\n",
        "df_features['Is_Night'] = ((df_features['Hour'] >= 22) | (df_features['Hour'] <= 6)).astype(int)\n",
        "df_features['Is_Peak_Fraud_Hour'] = ((df_features['Hour'] >= 1) & (df_features['Hour'] <= 4)).astype(int)\n",
        "df_features['Is_Weekend'] = (df_features['Day'] % 7 >= 5).astype(int)  # Assuming day 0 = Monday\n",
        "\n",
        "# Amount-based features\n",
        "print(\"Creating amount-based features...\")\n",
        "df_features['Amount_Log'] = np.log1p(df_features['Amount'])  # log(1+x) to handle zeros\n",
        "df_features['Is_Zero_Amount'] = (df_features['Amount'] == 0).astype(int)\n",
        "\n",
        "# Amount percentile features\n",
        "df_features['Amount_Percentile'] = pd.qcut(df_features['Amount'], q=10, labels=False, duplicates='drop')\n",
        "\n",
        "print(\"Feature engineering completed!\")\n",
        "print(f\"New dataset shape: {df_features.shape}\")\n",
        "print(f\"Added {df_features.shape[1] - df.shape[1]} new features\")\n",
        "\n",
        "# Display new features\n",
        "new_features = ['Hour', 'Day', 'Is_Night', 'Is_Peak_Fraud_Hour', 'Is_Weekend',\n",
        "                'Amount_Log', 'Is_Zero_Amount', 'Amount_Percentile']\n",
        "print(f\"New features: {new_features}\")\n",
        "\n",
        "# Analyze new features effectiveness\n",
        "print(\"\\nNew features analysis:\")\n",
        "for feature in ['Is_Night', 'Is_Peak_Fraud_Hour', 'Is_Zero_Amount']:\n",
        "    fraud_rate_feature = df_features[df_features[feature] == 1]['Class'].mean() * 100\n",
        "    normal_rate_feature = df_features[df_features[feature] == 0]['Class'].mean() * 100\n",
        "    print(f\"• {feature}: Fraud rate when True: {fraud_rate_feature:.4f}%, when False: {normal_rate_feature:.4f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7H_FWfTNQAL",
        "outputId": "d80cc7bb-8e19-4f1f-cba8-f9e9fb6605d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "FEATURE SELECTION EXPERIMENTS\n",
            "==================================================\n",
            "Total features available: 36\n",
            "• Basic features: 2\n",
            "• PCA features: 28\n",
            "• Engineered features: 8\n",
            "\n",
            "Feature sets for experimentation:\n",
            "• all_features: 36 features\n",
            "• top_10_eda: 10 features\n",
            "• pca_only: 28 features\n",
            "• basic_engineered: 8 features\n",
            "\n",
            "Performing statistical feature selection...\n",
            "Top 15 features by f_classif:\n",
            " 1. V17: 30923.97\n",
            " 2. V14: 26719.61\n",
            " 3. V12: 19029.93\n",
            " 4. V10: 12697.85\n",
            " 5. V16: 10302.27\n",
            " 6. V3: 9755.68\n",
            " 7. V7: 8685.54\n",
            " 8. V11: 6447.91\n",
            " 9. V4: 4826.05\n",
            "10. V18: 3183.66\n",
            "11. V1: 2555.78\n",
            "12. V9: 2530.49\n",
            "13. V5: 2204.80\n",
            "14. V2: 2046.49\n",
            "15. V6: 548.24\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 4. FEATURE SELECTION EXPERIMENTS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FEATURE SELECTION EXPERIMENTS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Prepare feature sets for comparison\n",
        "pca_features = [col for col in df.columns if col.startswith('V')]\n",
        "basic_features = ['Time', 'Amount']\n",
        "engineered_features = new_features\n",
        "all_features = basic_features + pca_features + engineered_features\n",
        "\n",
        "# Remove target and temporary features\n",
        "features_to_remove = ['Class', 'Hour', 'Day']  # Keep Hour/Day info in binary features\n",
        "all_features = [f for f in all_features if f not in features_to_remove]\n",
        "\n",
        "print(f\"Total features available: {len(all_features)}\")\n",
        "print(f\"• Basic features: {len(basic_features)}\")\n",
        "print(f\"• PCA features: {len(pca_features)}\")\n",
        "print(f\"• Engineered features: {len(engineered_features)}\")\n",
        "\n",
        "# Feature set experiments\n",
        "feature_sets = {\n",
        "    'all_features': all_features,\n",
        "    'top_10_eda': EDA_INSIGHTS['top_features'],\n",
        "    'pca_only': pca_features,\n",
        "    'basic_engineered': basic_features + [f for f in engineered_features if f not in ['Hour', 'Day']]\n",
        "}\n",
        "\n",
        "print(\"\\nFeature sets for experimentation:\")\n",
        "for name, features in feature_sets.items():\n",
        "    print(f\"• {name}: {len(features)} features\")\n",
        "\n",
        "# Statistical feature selection\n",
        "print(\"\\nPerforming statistical feature selection...\")\n",
        "X_temp = df_features[all_features]\n",
        "y_temp = df_features['Class']\n",
        "\n",
        "# SelectKBest with f_classif\n",
        "selector_f = SelectKBest(score_func=f_classif, k=15)\n",
        "X_selected_f = selector_f.fit_transform(X_temp, y_temp)\n",
        "selected_features_f = [all_features[i] for i in selector_f.get_support(indices=True)]\n",
        "\n",
        "print(f\"Top 15 features by f_classif:\")\n",
        "feature_scores = list(zip(selected_features_f, selector_f.scores_[selector_f.get_support()]))\n",
        "feature_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "for i, (feature, score) in enumerate(feature_scores):\n",
        "    print(f\"{i+1:2d}. {feature}: {score:.2f}\")\n",
        "\n",
        "# Add statistical selection to feature sets\n",
        "feature_sets['statistical_top15'] = selected_features_f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NN8ZQEKsNXUh",
        "outputId": "b5eef5d4-bde0-49b3-d58f-89baef068eee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "DATA SPLITTING\n",
            "==================================================\n",
            "Final dataset for modeling:\n",
            "• Features shape: (283726, 36)\n",
            "• Target shape: (283726,)\n",
            "• Class distribution: Counter({0: 283253, 1: 473})\n",
            "\n",
            "Data splits:\n",
            "• Train: 198,721 samples (0.1666% fraud)\n",
            "• Validation: 42,446 samples (0.1673% fraud)\n",
            "• Test: 42,559 samples (0.1668% fraud)\n",
            "\n",
            "Fraud rate consistency check:\n",
            "• Original: 0.1667%\n",
            "• Train: 0.1666%\n",
            "• Validation: 0.1673%\n",
            "• Test: 0.1668%\n",
            "✓ Stratification successful!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 5. DATA SPLITTING STRATEGY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"DATA SPLITTING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Prepare final dataset for splitting\n",
        "X = df_features[all_features]\n",
        "y = df_features['Class']\n",
        "\n",
        "print(f\"Final dataset for modeling:\")\n",
        "print(f\"• Features shape: {X.shape}\")\n",
        "print(f\"• Target shape: {y.shape}\")\n",
        "print(f\"• Class distribution: {Counter(y)}\")\n",
        "\n",
        "# Stratified train/validation/test split\n",
        "# 70% train, 15% validation, 15% test\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    X, y, test_size=0.15, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.176, random_state=42, stratify=y_temp  # 0.176 ≈ 0.15/0.85\n",
        ")\n",
        "\n",
        "print(f\"\\nData splits:\")\n",
        "print(f\"• Train: {X_train.shape[0]:,} samples ({y_train.mean()*100:.4f}% fraud)\")\n",
        "print(f\"• Validation: {X_val.shape[0]:,} samples ({y_val.mean()*100:.4f}% fraud)\")\n",
        "print(f\"• Test: {X_test.shape[0]:,} samples ({y_test.mean()*100:.4f}% fraud)\")\n",
        "\n",
        "# Verify stratification worked\n",
        "print(f\"\\nFraud rate consistency check:\")\n",
        "print(f\"• Original: {y.mean()*100:.4f}%\")\n",
        "print(f\"• Train: {y_train.mean()*100:.4f}%\")\n",
        "print(f\"• Validation: {y_val.mean()*100:.4f}%\")\n",
        "print(f\"• Test: {y_test.mean()*100:.4f}%\")\n",
        "print(\"✓ Stratification successful!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEHumPKaNkGI",
        "outputId": "dec5738b-506b-41b8-9ab6-e120958ae24d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "FEATURE SCALING\n",
            "==================================================\n",
            "Features to scale: ['Time', 'Amount', 'Amount_Log', 'Amount_Percentile']\n",
            "\n",
            "Applying standard scaling...\n",
            "Scaling results for standard:\n",
            "  • Time: std 47468.0176 → 1.0000\n",
            "  • Amount: std 242.3625 → 1.0000\n",
            "  • Amount_Log: std 1.6569 → 1.0000\n",
            "\n",
            "Applying robust scaling...\n",
            "Scaling results for robust:\n",
            "  • Time: std 47468.0176 → 0.5576\n",
            "  • Amount: std 242.3625 → 3.3582\n",
            "  • Amount_Log: std 1.6569 → 0.6712\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 6. FEATURE SCALING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FEATURE SCALING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Features that need scaling (PCA features V1-V28 are already scaled)\n",
        "features_to_scale = ['Time', 'Amount', 'Amount_Log', 'Hour', 'Amount_Percentile']\n",
        "features_to_scale = [f for f in features_to_scale if f in X_train.columns]\n",
        "\n",
        "print(f\"Features to scale: {features_to_scale}\")\n",
        "\n",
        "# Initialize scalers\n",
        "scalers = {\n",
        "    'standard': StandardScaler(),\n",
        "    'robust': RobustScaler()\n",
        "}\n",
        "\n",
        "# Prepare scaled datasets\n",
        "scaled_datasets = {}\n",
        "\n",
        "for scaler_name, scaler in scalers.items():\n",
        "    print(f\"\\nApplying {scaler_name} scaling...\")\n",
        "\n",
        "    # Fit on training data only\n",
        "    X_train_scaled = X_train.copy()\n",
        "    X_val_scaled = X_val.copy()\n",
        "    X_test_scaled = X_test.copy()\n",
        "\n",
        "    # Scale specified features\n",
        "    X_train_scaled[features_to_scale] = scaler.fit_transform(X_train[features_to_scale])\n",
        "    X_val_scaled[features_to_scale] = scaler.transform(X_val[features_to_scale])\n",
        "    X_test_scaled[features_to_scale] = scaler.transform(X_test[features_to_scale])\n",
        "\n",
        "    scaled_datasets[scaler_name] = {\n",
        "        'X_train': X_train_scaled,\n",
        "        'X_val': X_val_scaled,\n",
        "        'X_test': X_test_scaled,\n",
        "        'scaler': scaler\n",
        "    }\n",
        "\n",
        "    # Show scaling results\n",
        "    print(f\"Scaling results for {scaler_name}:\")\n",
        "    for feature in features_to_scale[:3]:  # Show first 3 features\n",
        "        orig_std = X_train[feature].std()\n",
        "        scaled_std = X_train_scaled[feature].std()\n",
        "        print(f\"  • {feature}: std {orig_std:.4f} → {scaled_std:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDpdczVzNqcf",
        "outputId": "aa434eaa-29fc-4c70-c5e1-a9c2117e0446"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "HANDLING CLASS IMBALANCE\n",
            "==================================================\n",
            "Original imbalance ratio: 1:384\n",
            "Training set class distribution: Counter({0: 198390, 1: 331})\n",
            "\n",
            "Applying none strategy...\n",
            "  • New class distribution: Counter({0: 198390, 1: 331})\n",
            "  • New ratio: 1:599.4\n",
            "  • Dataset size: 198,721 samples\n",
            "\n",
            "Applying smote_conservative strategy...\n",
            "  • New class distribution: Counter({0: 198390, 1: 19839})\n",
            "  • New ratio: 1:10.0\n",
            "  • Dataset size: 218,229 samples\n",
            "\n",
            "Applying smote_moderate strategy...\n",
            "  • New class distribution: Counter({0: 198390, 1: 39678})\n",
            "  • New ratio: 1:5.0\n",
            "  • Dataset size: 238,068 samples\n",
            "\n",
            "Applying smote_aggressive strategy...\n",
            "  • New class distribution: Counter({0: 198390, 1: 99195})\n",
            "  • New ratio: 1:2.0\n",
            "  • Dataset size: 297,585 samples\n",
            "\n",
            "Applying smote_balanced strategy...\n",
            "  • New class distribution: Counter({0: 198390, 1: 198390})\n",
            "  • New ratio: 1:1.0\n",
            "  • Dataset size: 396,780 samples\n",
            "\n",
            "Applying undersampling strategy...\n",
            "  • New class distribution: Counter({0: 3310, 1: 331})\n",
            "  • New ratio: 1:10.0\n",
            "  • Dataset size: 3,641 samples\n",
            "\n",
            "Applying smote_enn strategy...\n",
            "  • New class distribution: Counter({0: 198072, 1: 39678})\n",
            "  • New ratio: 1:5.0\n",
            "  • Dataset size: 237,750 samples\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 7. HANDLING CLASS IMBALANCE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"HANDLING CLASS IMBALANCE\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(f\"Original imbalance ratio: 1:{EDA_INSIGHTS['imbalance_ratio']:.0f}\")\n",
        "print(f\"Training set class distribution: {Counter(y_train)}\")\n",
        "\n",
        "# Define resampling strategies\n",
        "resampling_strategies = {\n",
        "    'none': None,\n",
        "    'smote_conservative': SMOTE(sampling_strategy=0.1, random_state=42),    # 1:10 ratio\n",
        "    'smote_moderate': SMOTE(sampling_strategy=0.2, random_state=42),        # 1:5 ratio\n",
        "    'smote_aggressive': SMOTE(sampling_strategy=0.5, random_state=42),      # 1:2 ratio\n",
        "    'smote_balanced': SMOTE(sampling_strategy=1.0, random_state=42),        # 1:1 ratio\n",
        "    'undersampling': RandomUnderSampler(sampling_strategy=0.1, random_state=42),\n",
        "    'smote_enn': SMOTEENN(sampling_strategy=0.2, random_state=42)\n",
        "}\n",
        "\n",
        "# Prepare resampled datasets (using standard scaling)\n",
        "X_train_base = scaled_datasets['standard']['X_train']\n",
        "resampled_datasets = {}\n",
        "\n",
        "for strategy_name, strategy in resampling_strategies.items():\n",
        "    print(f\"\\nApplying {strategy_name} strategy...\")\n",
        "\n",
        "    if strategy is None:\n",
        "        # No resampling\n",
        "        X_resampled = X_train_base.copy()\n",
        "        y_resampled = y_train.copy()\n",
        "    else:\n",
        "        # Apply resampling\n",
        "        X_resampled, y_resampled = strategy.fit_resample(X_train_base, y_train)\n",
        "\n",
        "    resampled_datasets[strategy_name] = {\n",
        "        'X_train': X_resampled,\n",
        "        'y_train': y_resampled\n",
        "    }\n",
        "\n",
        "    # Show results\n",
        "    class_counts = Counter(y_resampled)\n",
        "    if class_counts[1] > 0:\n",
        "        new_ratio = class_counts[0] / class_counts[1]\n",
        "        print(f\"  • New class distribution: {class_counts}\")\n",
        "        print(f\"  • New ratio: 1:{new_ratio:.1f}\")\n",
        "        print(f\"  • Dataset size: {len(y_resampled):,} samples\")\n",
        "    else:\n",
        "        print(f\"  • No fraud samples in this strategy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpFqjTRHNzX8",
        "outputId": "3359e05f-8618-4d5a-d7a7-cfb1f2062463"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "PREPROCESSING PIPELINE SUMMARY\n",
            "==================================================\n",
            "Available preprocessing configurations:\n",
            "• feature_sets: ['all_features', 'top_10_eda', 'pca_only', 'basic_engineered', 'statistical_top15']\n",
            "• scaling_methods: ['standard', 'robust']\n",
            "• resampling_strategies: ['none', 'smote_conservative', 'smote_moderate', 'smote_aggressive', 'smote_balanced', 'undersampling', 'smote_enn']\n",
            "\n",
            "Total possible combinations: 70\n",
            "\n",
            "Recommended configurations for modeling:\n",
            "1. Features: top_10_eda, Scaling: standard, Resampling: smote_moderate\n",
            "2. Features: statistical_top15, Scaling: standard, Resampling: smote_conservative\n",
            "3. Features: all_features, Scaling: robust, Resampling: smote_aggressive\n",
            "4. Features: basic_engineered, Scaling: standard, Resampling: smote_balanced\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 8. PREPROCESSING PIPELINE SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PREPROCESSING PIPELINE SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Summarize all preprocessing options\n",
        "preprocessing_options = {\n",
        "    'feature_sets': list(feature_sets.keys()),\n",
        "    'scaling_methods': list(scalers.keys()),\n",
        "    'resampling_strategies': list(resampling_strategies.keys())\n",
        "}\n",
        "\n",
        "print(\"Available preprocessing configurations:\")\n",
        "for category, options in preprocessing_options.items():\n",
        "    print(f\"• {category}: {options}\")\n",
        "\n",
        "total_combinations = len(feature_sets) * len(scalers) * len(resampling_strategies)\n",
        "print(f\"\\nTotal possible combinations: {total_combinations}\")\n",
        "\n",
        "# Recommend best combinations for modeling\n",
        "recommended_configs = [\n",
        "    ('top_10_eda', 'standard', 'smote_moderate'),\n",
        "    ('statistical_top15', 'standard', 'smote_conservative'),\n",
        "    ('all_features', 'robust', 'smote_aggressive'),\n",
        "    ('basic_engineered', 'standard', 'smote_balanced')\n",
        "]\n",
        "\n",
        "print(f\"\\nRecommended configurations for modeling:\")\n",
        "for i, (features, scaling, resampling) in enumerate(recommended_configs, 1):\n",
        "    print(f\"{i}. Features: {features}, Scaling: {scaling}, Resampling: {resampling}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ht201_KLN3s0",
        "outputId": "836b99b7-3a95-460f-8eb9-78204dbb0ba0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "SAVING PREPROCESSED DATA\n",
            "==================================================\n",
            "Creating final preprocessed dataset:\n",
            "• Features: ['V17', 'V14', 'V3', 'V10', 'V12', 'V16', 'V7', 'V11', 'V4', 'V18']\n",
            "• Scaling: standard\n",
            "• Resampling: smote_moderate\n",
            "\n",
            "Final dataset shapes:\n",
            "• X_train_resampled: (238068, 10)\n",
            "• X_val_final: (42446, 10)\n",
            "• X_test_final: (42559, 10)\n",
            "• Final class distribution: Counter({0: 198390, 1: 39678})\n",
            "✓ Preprocessing objects ready for saving\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 9. SAVE PREPROCESSED DATA\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"SAVING PREPROCESSED DATA\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Create a sample preprocessed dataset for modeling (recommended config 1)\n",
        "best_features = feature_sets['top_10_eda']\n",
        "best_scaling = 'standard'\n",
        "best_resampling = 'smote_moderate'\n",
        "\n",
        "print(f\"Creating final preprocessed dataset:\")\n",
        "print(f\"• Features: {best_features}\")\n",
        "print(f\"• Scaling: {best_scaling}\")\n",
        "print(f\"• Resampling: {best_resampling}\")\n",
        "\n",
        "# Prepare final datasets\n",
        "X_train_final = scaled_datasets[best_scaling]['X_train'][best_features]\n",
        "X_val_final = scaled_datasets[best_scaling]['X_val'][best_features]\n",
        "X_test_final = scaled_datasets[best_scaling]['X_test'][best_features]\n",
        "\n",
        "# Apply resampling to training set\n",
        "X_train_resampled, y_train_resampled = resampling_strategies[best_resampling].fit_resample(\n",
        "    X_train_final, y_train\n",
        ")\n",
        "\n",
        "print(f\"\\nFinal dataset shapes:\")\n",
        "print(f\"• X_train_resampled: {X_train_resampled.shape}\")\n",
        "print(f\"• X_val_final: {X_val_final.shape}\")\n",
        "print(f\"• X_test_final: {X_test_final.shape}\")\n",
        "print(f\"• Final class distribution: {Counter(y_train_resampled)}\")\n",
        "\n",
        "# Save preprocessing objects for deployment\n",
        "preprocessing_objects = {\n",
        "    'feature_names': best_features,\n",
        "    'scaler': scalers[best_scaling],\n",
        "    'resampler': resampling_strategies[best_resampling],\n",
        "    'feature_sets': feature_sets,\n",
        "    'eda_insights': EDA_INSIGHTS\n",
        "}\n",
        "\n",
        "print(\"✓ Preprocessing objects ready for saving\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLlyQe2IN7pD",
        "outputId": "269a73b4-e6e7-491e-a210-8a669bcbb63d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "VALIDATION AND QUALITY CHECKS\n",
            "==================================================\n",
            "Data leakage checks:\n",
            "✓ Scaling fitted only on training data\n",
            "✓ Resampling applied only to training data\n",
            "✓ Feature selection based on training data only\n",
            "\n",
            "Feature correlation analysis (final dataset):\n",
            "High correlation pairs found (|r| > 0.8):\n",
            "  • V17 - V10: 0.831\n",
            "  • V17 - V12: 0.854\n",
            "  • V17 - V16: 0.902\n",
            "  • V17 - V18: 0.842\n",
            "  • V14 - V12: 0.859\n",
            "  • V14 - V11: 0.817\n",
            "  • V3 - V7: 0.826\n",
            "  • V10 - V12: 0.850\n",
            "  • V10 - V16: 0.814\n",
            "  • V10 - V7: 0.816\n",
            "  • V12 - V16: 0.845\n",
            "  • V12 - V11: 0.816\n",
            "\n",
            "Memory usage: 18.16 MB\n",
            "\n",
            "Quick feature importance check:\n",
            "Top 5 features by Random Forest importance:\n",
            "1. V14: 0.3501\n",
            "2. V17: 0.1542\n",
            "3. V10: 0.1487\n",
            "4. V12: 0.1062\n",
            "5. V11: 0.0697\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 10. VALIDATION AND QUALITY CHECKS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"VALIDATION AND QUALITY CHECKS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Check for data leakage\n",
        "print(\"Data leakage checks:\")\n",
        "print(\"✓ Scaling fitted only on training data\")\n",
        "print(\"✓ Resampling applied only to training data\")\n",
        "print(\"✓ Feature selection based on training data only\")\n",
        "\n",
        "# Check feature correlations in final dataset\n",
        "print(\"\\nFeature correlation analysis (final dataset):\")\n",
        "correlation_matrix = X_train_resampled.corr()\n",
        "high_corr_pairs = []\n",
        "\n",
        "for i in range(len(best_features)):\n",
        "    for j in range(i+1, len(best_features)):\n",
        "        corr_val = abs(correlation_matrix.iloc[i, j])\n",
        "        if corr_val > 0.8:\n",
        "            high_corr_pairs.append((best_features[i], best_features[j], corr_val))\n",
        "\n",
        "if high_corr_pairs:\n",
        "    print(\"High correlation pairs found (|r| > 0.8):\")\n",
        "    for pair in high_corr_pairs:\n",
        "        print(f\"  • {pair[0]} - {pair[1]}: {pair[2]:.3f}\")\n",
        "else:\n",
        "    print(\"✓ No high correlations found (|r| > 0.8)\")\n",
        "\n",
        "# Memory usage check\n",
        "memory_usage = X_train_resampled.memory_usage(deep=True).sum() / 1024**2\n",
        "print(f\"\\nMemory usage: {memory_usage:.2f} MB\")\n",
        "\n",
        "# Feature importance preview (quick Random Forest)\n",
        "print(\"\\nQuick feature importance check:\")\n",
        "rf_temp = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
        "rf_temp.fit(X_train_resampled, y_train_resampled)\n",
        "feature_importance = list(zip(best_features, rf_temp.feature_importances_))\n",
        "feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"Top 5 features by Random Forest importance:\")\n",
        "for i, (feature, importance) in enumerate(feature_importance[:5]):\n",
        "    print(f\"{i+1}. {feature}: {importance:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "016XbNXqLmnB",
        "outputId": "51cc6c3e-2d50-46bb-a1c1-576d3371e2f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "PREPROCESSING COMPLETED SUCCESSFULLY!\n",
            "==================================================\n",
            " PREPROCESSING SUMMARY:\n",
            "• original_samples: 63,472\n",
            "• cleaned_samples: 283,726\n",
            "• final_train_samples: 238,068\n",
            "• final_features: 10\n",
            "• final_fraud_rate: 16.67\n",
            "• preprocessing_combinations: 70\n",
            "\n",
            " READY FOR MODELING:\n",
            "• ✓ Data cleaned and preprocessed\n",
            "• ✓ Features engineered and selected\n",
            "• ✓ Class imbalance handled\n",
            "• ✓ Train/val/test splits prepared\n",
            "• ✓ Scaling applied\n",
            "• ✓ No data leakage\n",
            "\n",
            " DATASETS READY:\n",
            "• X_train_resampled, y_train_resampled (for training)\n",
            "• X_val_final, y_val (for validation)\n",
            "• X_test_final, y_test (for final testing)\n",
            "\n",
            " NEXT STEPS:\n",
            "1. Model training and comparison\n",
            "2. Hyperparameter tuning\n",
            "3. Model evaluation\n",
            "4. Deep learning model\n",
            "5. Model deployment\n",
            "\n",
            " Preprocessed data exported for modeling phase!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 11. PREPROCESSING COMPLETED\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PREPROCESSING COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "summary_stats = {\n",
        "    'original_samples': EDA_INSIGHTS['total_transactions'],\n",
        "    'cleaned_samples': len(df),\n",
        "    'final_train_samples': len(X_train_resampled),\n",
        "    'final_features': len(best_features),\n",
        "    'final_fraud_rate': y_train_resampled.mean() * 100,\n",
        "    'preprocessing_combinations': total_combinations\n",
        "}\n",
        "\n",
        "print(\" PREPROCESSING SUMMARY:\")\n",
        "for key, value in summary_stats.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"• {key}: {value:.2f}\")\n",
        "    else:\n",
        "        print(f\"• {key}: {value:,}\")\n",
        "\n",
        "print(\"\\n READY FOR MODELING:\")\n",
        "print(\"• ✓ Data cleaned and preprocessed\")\n",
        "print(\"• ✓ Features engineered and selected\")\n",
        "print(\"• ✓ Class imbalance handled\")\n",
        "print(\"• ✓ Train/val/test splits prepared\")\n",
        "print(\"• ✓ Scaling applied\")\n",
        "print(\"• ✓ No data leakage\")\n",
        "\n",
        "print(\"\\n DATASETS READY:\")\n",
        "print(\"• X_train_resampled, y_train_resampled (for training)\")\n",
        "print(\"• X_val_final, y_val (for validation)\")\n",
        "print(\"• X_test_final, y_test (for final testing)\")\n",
        "\n",
        "print(\"\\n NEXT STEPS:\")\n",
        "print(\"1. Model training and comparison\")\n",
        "print(\"2. Hyperparameter tuning\")\n",
        "print(\"3. Model evaluation\")\n",
        "print(\"4. Deep learning model\")\n",
        "print(\"5. Model deployment\")\n",
        "\n",
        "# Export key variables for next notebook\n",
        "PREPROCESSED_DATA = {\n",
        "    'X_train': X_train_resampled,\n",
        "    'y_train': y_train_resampled,\n",
        "    'X_val': X_val_final,\n",
        "    'y_val': y_val,\n",
        "    'X_test': X_test_final,\n",
        "    'y_test': y_test,\n",
        "    'feature_names': best_features,\n",
        "    'preprocessing_objects': preprocessing_objects\n",
        "}\n",
        "\n",
        "print(f\"\\n Preprocessed data exported for modeling phase!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "HdfkKNuUQJGB"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "data_to_save = {\n",
        "    'X_train': X_train_resampled,\n",
        "    'y_train': y_train_resampled,\n",
        "    'X_val': X_val_final,\n",
        "    'y_val': y_val,\n",
        "    'X_test': X_test_final,\n",
        "    'y_test': y_test,\n",
        "    'feature_names': best_features\n",
        "}\n",
        "pickle.dump(data_to_save, open('preprocessed_data.pkl', 'wb'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
